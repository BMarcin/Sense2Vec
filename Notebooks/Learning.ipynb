{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from poutyne.framework import Model, Experiment, OptimizerPolicy, sgdr_phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1010101011)\n",
    "random.seed(1010101011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"../data/postprocessed/ds_blogs.txt\"\n",
    "# dataset = \"testds.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DS(DataLoader):\n",
    "    def __init__(self, corpus_path, window_size, min_occurences):\n",
    "        self.corpus_path = corpus_path\n",
    "        \n",
    "        self.counter = Counter()\n",
    "        self.token2idx = {\n",
    "            \"<null>\": 0\n",
    "        }\n",
    "        \n",
    "        self.window_size = window_size\n",
    "        self.half_of_window_size = int(self.window_size / 2)\n",
    "        self.min_occurences = min_occurences\n",
    "        \n",
    "        self.sentences_len = 0\n",
    "        \n",
    "        with open(corpus_path) as f:\n",
    "            for sentence in tqdm(f, desc=\"Counting tokens\"):\n",
    "                self.sentences_len += 1\n",
    "                sentence = sentence.replace(\"\\n\", \"\")\n",
    "                for token in sentence.split():\n",
    "                    self.counter[token.lower()] += 1\n",
    "        ' build vocab '            \n",
    "        self.vocab = set([token for token in self.counter.keys() if self.counter[token] >= self.min_occurences])\n",
    "        \n",
    "        ' build token2idx '\n",
    "        for token in tqdm(self.vocab, desc=\"Building token2idx\"):\n",
    "            self.token2idx[token] = len(self.token2idx)\n",
    "            \n",
    "        print(\"Tokens: {}\".format(len(self.token2idx)))\n",
    "        \n",
    "        self.ds_x = []\n",
    "        self.ds_y = []\n",
    "        for inputs, output in tqdm(self.build_ds(), desc=\"Building dataset\"):\n",
    "            inputs, output = self.numericalize(inputs, output)\n",
    "            self.ds_x.append(inputs)\n",
    "            self.ds_y.append(output)\n",
    "    \n",
    "    def build_ds(self):\n",
    "        with open(self.corpus_path) as f:\n",
    "            for sentence in f:\n",
    "                sentence = sentence.replace(\"\\n\", \"\").lower()\n",
    "                \n",
    "                sent_splt = sentence.split()\n",
    "                \n",
    "                uniq_sentence = set(sent_splt)\n",
    "                \n",
    "                if uniq_sentence.issubset(self.vocab) and len(sent_splt) >= self.window_size:\n",
    "                    splitted = sent_splt\n",
    "                    sentence_splitted = ['<null>' for _ in range(self.half_of_window_size)] + \\\n",
    "                        splitted + ['<null>' for _ in range(self.half_of_window_size)]\n",
    "                    \n",
    "                    index = len(sentence_splitted)\n",
    "                    while len(splitted) > 0:\n",
    "                        token = splitted.pop()\n",
    "                        \n",
    "                        inputs_left = sentence_splitted[index - 2 * self.half_of_window_size - 1:index - 2 * self.half_of_window_size + self.half_of_window_size - 1]\n",
    "                        inputs_right = sentence_splitted[index - 2 * self.half_of_window_size + self.half_of_window_size:index - 2 * self.half_of_window_size + self.half_of_window_size + self.half_of_window_size]\n",
    "                        \n",
    "                        index = index - 1\n",
    "\n",
    "                        yield inputs_left + inputs_right, token\n",
    "    \n",
    "    def numericalize(self, inputs, output):\n",
    "        return [self.token2idx[token] for token in inputs], self.token2idx[output]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.ds_x[index]).long(), torch.tensor(self.ds_y[index]).long()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ds_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "bs = 5000\n",
    "seq_len = 7\n",
    "target_vectors = 100\n",
    "embedding_size=30\n",
    "epochs = 5\n",
    "device = torch.device(\"cuda:4\")\n",
    "minimal_token_occurences = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting tokens: 8257486it [00:49, 167305.13it/s]\n",
      "Building token2idx: 100%|██████████| 95067/95067 [00:00<00:00, 1323890.39it/s]\n",
      "Building dataset: 39823it [00:00, 59383.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: 95068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building dataset: 71289233it [04:36, 257794.72it/s]\n"
     ]
    }
   ],
   "source": [
    "ds = DS(dataset, seq_len, minimal_token_occurences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DL = DataLoader(dataset=ds, batch_size=bs, num_workers=4, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sense2VecCBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, vectors, sequence_length):\n",
    "        super(Sense2VecCBOW, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.vectors = vectors\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.fc_in = nn.Linear(embedding_size * (sequence_length - 1), vectors)\n",
    "        self.fc_out = nn.Linear(vectors, vocab_size)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_range = 0.1\n",
    "        self.fc_in.weight.data.uniform_(-init_range, init_range)\n",
    "        self.fc_out.weight.data.uniform_(-init_range, init_range)\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.fc_out.weight.cpu().detach().tolist()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.fc_in(x.reshape(len(x), -1))\n",
    "        # x = torch.relu(x)\n",
    "        x = self.fc_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sense2VecCBOW(\n",
    "        len(ds.token2idx),\n",
    "        embedding_size,\n",
    "        target_vectors,\n",
    "        seq_len\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = OptimizerPolicy(\n",
    "    sgdr_phases(len(DL), epochs, (3e-2, 3e-4), 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(\n",
    "    '../experiments/t6',\n",
    "    model,\n",
    "    optimizer=optimizer,\n",
    "    loss_function=criterion,\n",
    "    batch_metrics=['accuracy'],\n",
    "    monitor_metric='acc',\n",
    "    monitor_mode='max',\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 1107.05s Step 14258/14258: loss: 4.202043, acc: 32.395840\n",
      "Epoch 1: acc improved from -inf to 32.39584, saving file to ../experiments/t6/checkpoint_epoch_1.ckpt\n",
      "Epoch 2/5 1100.82s Step 14258/14258: loss: 4.038350, acc: 33.773256\n",
      "Epoch 2: acc improved from 32.39584 to 33.77326, saving file to ../experiments/t6/checkpoint_epoch_2.ckpt\n",
      "Epoch 3/5 1136.27s Step 14258/14258: loss: 4.014506, acc: 33.949107\n",
      "Epoch 3: acc improved from 33.77326 to 33.94911, saving file to ../experiments/t6/checkpoint_epoch_3.ckpt\n",
      "Epoch 4/5 ETA 923s Step 4416/14258: loss: 3.941869, acc: 35.0000008"
     ]
    }
   ],
   "source": [
    "experiment.train(DL, \n",
    "                 epochs=epochs,\n",
    "#                  callbacks=[policy]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
